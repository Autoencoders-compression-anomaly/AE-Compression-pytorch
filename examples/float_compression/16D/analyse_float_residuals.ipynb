{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not a ROOT file (starts with b'\\x80\\x04\\x95w' instead of 'root')\n   in file: ../../processed_data/aod/uncompressed_all_jets_train.pkl",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-32867e7004d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;31m#test = pd.read_pickle(BIN + 'processed_data/aod/uncompressed_all_jets_test.pkl')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;31m#n_features = len(train.loc[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpair_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBIN\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'processed_data/aod/uncompressed_all_jets_train.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBIN\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'processed_data/aod/compressed_all_jets_train.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpair_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBIN\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'processed_data/aod/uncompressed_all_jets_test.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBIN\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'processed_data/aod/compressed_all_jets_test.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-32867e7004d2>\u001b[0m in \u001b[0;36mpair_datasets\u001b[0;34m(filename, compressedfilename)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpair_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompressedfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muproot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m#Compressed variables have a c in them somewhere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/uproot/rootio.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, localsource, xrootdsource, httpsource, **options)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mopenfcn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocalsource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mROOTDirectory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopenfcn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0m_bytesid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"root\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/uproot/rootio.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(source, *args, **options)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mmagic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfVersion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mROOTDirectory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmagic\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34mb\"root\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"not a ROOT file (starts with {0} instead of 'root')\\n   in file: {1}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfVersion\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1000000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                     \u001b[0mfBEGIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfEND\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfSeekFree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfNbytesFree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfNbytesName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfUnits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfCompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfSeekInfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfNbytesInfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfUUID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mROOTDirectory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format2_small\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not a ROOT file (starts with b'\\x80\\x04\\x95w' instead of 'root')\n   in file: ../../processed_data/aod/uncompressed_all_jets_train.pkl"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "BIN = '../../'\n",
    "sys.path.append(BIN)\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import my_matplotlib_style as ms\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%matplotlib inline\n",
    "import sys\n",
    "BIN = '../../'\n",
    "sys.path.append(BIN)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import my_matplotlib_style as ms\n",
    "\n",
    "from fastai import data_block, basic_train, basic_data\n",
    "from fastai.callbacks import ActivationStats\n",
    "import fastai\n",
    "\n",
    "import uproot\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rc_file(BIN + 'my_matplotlib_rcparams')\n",
    "\n",
    "from nn_utils import AE_big, AE_3D_200\n",
    "from utils import plot_activations\n",
    "\n",
    "def pair_datasets(filename, compressedfilename):\n",
    "    #file = uproot.open(filename)\n",
    "\n",
    "    #Compressed variables have a c in them somewhere\n",
    "    #filec = uproot.open(compressedfilename)\n",
    "\n",
    "    feventNumber = file[\"CollectionTree/EventInfoAux.eventNumber\"].array()\n",
    "    fceventNumber = filec[\"CollectionTree/EventInfoAux.eventNumber\"].array()\n",
    "\n",
    "    \n",
    "    \n",
    "    fm = file[\"CollectionTree/AntiKt4EMTopoJetsAux.m\"].array()\n",
    "    fphi = file[\"CollectionTree/AntiKt4EMTopoJetsAux.phi\"].array()\n",
    "    feta = file[\"CollectionTree/AntiKt4EMTopoJetsAux.eta\"].array()\n",
    "    fpt = file[\"CollectionTree/AntiKt4EMTopoJetsAux.pt\"].array()\n",
    "    facteta = file[\"CollectionTree/AntiKt4EMTopoJetsAux.ActiveArea4vec_eta\"].array()\n",
    "    factphi = file[\"CollectionTree/AntiKt4EMTopoJetsAux.ActiveArea4vec_phi\"].array()\n",
    "    factm = file[\"CollectionTree/AntiKt4EMTopoJetsAux.ActiveArea4Vec_m\"].array()\n",
    "    factpt = file[\"CollectionTree/AntiKt4EMTopoJetsAux.ActiveArea4Vec_pt\"].array()\n",
    "    fjvt = file[\"CollectionTree/AntiKt4EMTopoJetsAux.Jvt\"].array()\n",
    "    fjvfcorr = file[\"CollectionTree/AntiKt4EMTopoJetsAux.JVFCorr\"].array()\n",
    "    ffracsamplingmax = file[\"CollectionTree/AntiKt4EMTopoJetsAux.FracSamplingMax\"].array()\n",
    "    ffracsamplingmaxindex = file[\"CollectionTree/AntiKt4EMTopoJetsAux.FracSamplingMaxIndex\"].array()\n",
    "    fwidth = file[\"CollectionTree/AntiKt4EMTopoJetsAux.Width\"].array()\n",
    "    femfrac = file[\"CollectionTree/AntiKt4EMTopoJetsAux.EMFrac\"].array()\n",
    "    ftiming = file[\"CollectionTree/AntiKt4EMTopoJetsAux.Timing\"].array()\n",
    "    \n",
    "    fcm = filec[\"CollectionTree/AntiKt4EMTopoJetsAux.m\"].array()\n",
    "    fcphi = filec[\"CollectionTree/AntiKt4EMTopoJetsAux.phi\"].array()\n",
    "    fceta = filec[\"CollectionTree/AntiKt4EMTopoJetsAux.eta\"].array()\n",
    "    fcpt = filec[\"CollectionTree/AntiKt4EMTopoJetsAux.pt\"].array()\n",
    "    factetac = filec[\"CollectionTree/AntiKt4EMTopoJetsAux.ActiveArea4vec_eta\"].array()\n",
    "    factphic = filec[\"CollectionTree/AntiKt4EMTopoJetsAux.ActiveArea4vec_phi\"].array()\n",
    "    factmc = filec[\"CollectionTree/AntiKt4EMTopoJetsAux.ActiveArea4Vec_m\"].array()\n",
    "    factptc = filec[\"CollectionTree/AntiKt4EMTopoJetsAux.ActiveArea4Vec_pt\"].array()\n",
    "    fjvtc = filec[\"CollectionTree/AntiKt4EMTopoJetsAux.Jvt\"].array()\n",
    "    fjvfcorrc = filec[\"CollectionTree/AntiKt4EMTopoJetsAux.JVFCorr\"].array()\n",
    "    ffracsamplingmaxc = filec[\"CollectionTree/AntiKt4EMTopoJetsAux.FracSamplingMax\"].array()\n",
    "    ffracsamplingmaxindexc = filec[\"CollectionTree/AntiKt4EMTopoJetsAux.FracSamplingMaxIndex\"].array()\n",
    "    fwidthc = filec[\"CollectionTree/AntiKt4EMTopoJetsAux.Width\"].array()\n",
    "    femfracc = filec[\"CollectionTree/AntiKt4EMTopoJetsAux.EMFrac\"].array()\n",
    "    ftimingc = filec[\"CollectionTree/AntiKt4EMTopoJetsAux.Timing\"].array()\n",
    "\n",
    "    print(fm[0])\n",
    "    print(fphi[0])\n",
    "    print(feta[0])\n",
    "    print(fpt[0])\n",
    "    \n",
    "    fevents = [[feventNumber[i], fm[i], fphi[i], feta[i], fpt[i], facteta[i],  factm[i],factphi[i], factpt[i], fjvt[i],fjvfcorr[i],ffracsamplingmax[i],ffracsamplinmaxide[i],fwidth[i],femfrac[i],ftiming[i]] for i in range(0,len(feventNumber))]\n",
    "\n",
    "    fcevents = [[fceventNumber[i], fcm[i], fcphi[i], fceta[i], fcpt[i], factetac[i], factmc[i],factphic[i],  factptc[i], fjvtc[i],fjvfcorrc[i],ffracsamplingmaxc[i],ffracsamplinmaxindexc[i],fwidthc[i],femfracc[i],ftimingc[i]] for i in range(0,len(fceventNumber))]\n",
    "\n",
    "    fevents.sort()\n",
    "    fcevents.sort()\n",
    "\n",
    "    if len(fevents) != len(fcevents):\n",
    "        raise Exception(\"Number of events not matching\")\n",
    "\n",
    "    for i in range(0,len(fevents)):\n",
    "        if fevents[i][0] != fcevents[i][0]:\n",
    "            raise Exception(\"Events are not matching\")\n",
    "\n",
    "    #Match jets by deltaR < 0.05\n",
    "\n",
    "    def proper_phi(phi):\n",
    "        if phi < -np.pi:\n",
    "            return phi+2*np.pi\n",
    "        if phi > np.pi:\n",
    "            return phi-2*np.pi\n",
    "        return phi\n",
    "\n",
    "    def deltaR(eta1, eta2, phi1, phi2):\n",
    "        deta = eta1-eta2\n",
    "        dphi = proper_phi(phi1-phi2)\n",
    "        return np.sqrt(deta**2+dphi**2)\n",
    "\n",
    "    #Iterate through every event and sort jets according to the uncompressed data\n",
    "\n",
    "    compressed_events = []\n",
    "\n",
    "    #Iterate through events\n",
    "    for ei in range(0,len(fevents)):\n",
    "\n",
    "        if ei % 1000 == 0:\n",
    "            print(str(int(ei/len(fevents)*100)) + \"%\")\n",
    "\n",
    "        cm = []\n",
    "        cphi = []\n",
    "        ceta = []\n",
    "        cpt = []\n",
    "        cacteta = []\n",
    "        cactm = []\n",
    "        cactphi = []\n",
    "        cactpt = []\n",
    "        cjvt = []\n",
    "        cjvfcorr = []\n",
    "        cfracsamplingmax = []\n",
    "        cfracsamplingmaxindex = []\n",
    "        cwidth = []\n",
    "        cemfrac = []\n",
    "        ctiming = []\n",
    "\n",
    "        #Iterate through jets in uncompressed event\n",
    "        for ji in range(0,len(fevents[ei][1])):\n",
    "            for jci in range(0,len(fcevents[ei][1])):\n",
    "                dR = deltaR(fevents[ei][3][ji], fcevents[ei][3][jci], fevents[ei][2][ji], fcevents[ei][2][jci])\n",
    "\n",
    "                if dR < 0.05:\n",
    "                    cm.append(fcevents[ei][1][jci])\n",
    "                    cphi.append(fcevents[ei][2][jci])\n",
    "                    ceta.append(fcevents[ei][3][jci])\n",
    "                    cpt.append(fcevents[ei][4][jci])\n",
    "                    cacteta.append(fcevents[ei][5][jci])\n",
    "                    cactm.append(fcevents[ei][6][jci])\n",
    "                    cactphi.append(fcevents[ei][7][jci])\n",
    "                    cactpt.append(fcevents[ei][8][jci])\n",
    "                    cjvt.append(fcevents[ei][9][jci])\n",
    "                    cjvfcorr.append(fcevents[ei][10][jci])\n",
    "                    cfracsamplingmax.append(fcevents[ei][11][jci])\n",
    "                    cfracsamplingmaxindex.append(fcevents[ei][12][jci])\n",
    "                    cwidth.append(fcevents[ei][13][jci])\n",
    "                    cemfrac.append(fcevents[ei][14][jci])\n",
    "                    ctiming.append(fcevents[ei][15][jci])\n",
    "\n",
    "                    #fcevents[ei][1] = np.delete(fcevents[ei][1],jci)\n",
    "                    #fcevents[ei][2] = np.delete(fcevents[ei][2],jci)\n",
    "                    #fcevents[ei][3] = np.delete(fcevents[ei][3],jci)\n",
    "                    #fcevents[ei][4] = np.delete(fcevents[ei][4],jci)\n",
    "\n",
    "        if len(fevents[ei][1]) == len(cm):\t\n",
    "            compressed_events.append([fevents[ei][0], cm, cphi, ceta, cpt])\n",
    "        else:\n",
    "            print(\"Jets not matched in event: \" + str(fevents[ei][0]))\n",
    "            \n",
    "    return fevents, compressed_events\n",
    "\n",
    "# Load data\n",
    "#train = pd.read_pickle(BIN + 'processed_data/aod/uncompressed_all_jets_train.pkl')\n",
    "#test = pd.read_pickle(BIN + 'processed_data/aod/uncompressed_all_jets_test.pkl')\n",
    "#n_features = len(train.loc[0])\n",
    "train, trainc = pair_datasets(BIN + 'processed_data/aod/uncompressed_all_jets_train.pkl', BIN + 'processed_data/aod/compressed_all_jets_train.pkl')\n",
    "test, testc = pair_datasets(BIN + 'processed_data/aod/uncompressed_all_jets_test.pkl', BIN + 'processed_data/aod/compressed_all_jets_test.pkl')\n",
    "\n",
    "#Remove zero mass jets\n",
    "\n",
    "train = train.loc[train[\"m\"] > 0.08]\n",
    "test = test.loc[test[\"m\"] > 0.08]\n",
    "\n",
    "train = train.loc[train[\"m\"] < 15000]\n",
    "test = test.loc[test[\"m\"] < 15000]\n",
    "\n",
    "#train = train.loc[train[\"pt\"] > 10000]\n",
    "#test = test.loc[test[\"pt\"] > 10000]\n",
    "\n",
    "#train = train.loc[train[\"pt\"] < 25000]\n",
    "#test = test.loc[test[\"pt\"] < 25000]\n",
    "\n",
    "#train = train.loc[train[\"pt\"] > 25000]#.iloc(:1)\n",
    "#test = test.loc[test[\"pt\"] > 25000]\n",
    "\n",
    "#Shuffle datasets\n",
    "train = train.sample(frac=1)\n",
    "test = test.sample(frac=1)\n",
    "\n",
    "# Normalize\n",
    "\n",
    "# Custom normalization for AOD data\n",
    "eta_div = 5\n",
    "emfrac_div = 1.6\n",
    "negE_div = 1.6\n",
    "phi_div = 3\n",
    "m_div = 1.8\n",
    "width_div = .6\n",
    "N90_div = 20\n",
    "timing_div = 40\n",
    "hecq_div = 1\n",
    "centerlambda_div = 2\n",
    "secondlambda_div = 1\n",
    "secondR_div = .6\n",
    "larqf_div = 2.5\n",
    "pt_div = 1.2\n",
    "centroidR_div = 0.8\n",
    "area4vecm_div = 0.18\n",
    "area4vecpt_div = 0.7\n",
    "area4vec_div = 0.8\n",
    "Oot_div = 0.3\n",
    "larq_div = 0.6\n",
    "\n",
    "log_add = 100\n",
    "log_sub = 2\n",
    "\n",
    "centroidR_sub = 3\n",
    "\n",
    "area4vecm_sub = 0.15\n",
    "\n",
    "\n",
    "m_add = 1\n",
    "pt_sub = 1.3\n",
    "eta_div = 5\n",
    "phi_div = 3\n",
    "\n",
    "branches = [\"ActiveArea4vec_eta\", \"ActiveArea4vec_m\", \"ActiveArea4vec_phi\", \"ActiveArea4vec_pt\", \"Jvt\", \"JVFCorr\" ,\"JvtRpt\", \"FracSamplingMax\", \"FracSamplingMaxIndex\", \"Width\", \"EMFrac\",\"Timing\"]\n",
    "\n",
    "means = [train[k].mean() for k in branches]\n",
    "stds = [train[k].std() for k in branches]\n",
    "\n",
    "\n",
    "\n",
    "for data in [train,test]:\n",
    "    #data['m'] = (np.log10(data['m'])-3.6)\n",
    "    data['m'] = np.log10(data['m'] + m_add) / m_div -1.4\n",
    "    data['eta'] = data['eta'] / eta_div\n",
    "    \n",
    "    data['phi'] = data['phi'] / phi_div\n",
    "    \n",
    "    #data['pt'] = (np.log10(data['pt']/1000-10)) -0.7\n",
    "    data['pt'] = (np.log10(data['pt']) - pt_sub) / pt_div -2.4\n",
    "    \n",
    "    #data['Timing'] = np.log10(data['Timing'])\n",
    "    \n",
    "    for k in range(0,len(branches)):\n",
    "        data[branches[k]] = (data[branches[k]] - means[k])/stds[k]\n",
    "    \n",
    "    #data['Timing'] = data['Timing']/50\n",
    "    \n",
    "    #data['FracSamplingMaxIndex'] = data['FracSamplingMaxIndex']/20\n",
    "    \n",
    "\n",
    "\n",
    "train = train.loc[train[\"m\"] > 0]\n",
    "test = test.loc[test[\"m\"] > 0]\n",
    "\n",
    "train = train.drop(\"JvtRpt\",axis=1)\n",
    "test = test.drop(\"JvtRpt\",axis=1)\n",
    "\n",
    "print(means)\n",
    "\n",
    "del means[6:7]\n",
    "del stds[6:7]\n",
    "\n",
    "print(means)\n",
    "\n",
    "train_x = train\n",
    "test_x = test\n",
    "train_y = train_x  # y = x since we are building and AE\n",
    "test_y = test_x\n",
    "\n",
    "train_ds = TensorDataset(torch.tensor(train_x.values), torch.tensor(train_y.values))\n",
    "valid_ds = TensorDataset(torch.tensor(test_x.values), torch.tensor(test_y.values))\n",
    "\n",
    "# Load data\n",
    "#trainc = pd.read_pickle(BIN + 'processed_data/aod/compressed_all_jets_train.pkl')\n",
    "#testc = pd.read_pickle(BIN + 'processed_data/aod/compressed_all_jets_test.pkl')\n",
    "#n_features = len(train.loc[0])\n",
    "\n",
    "\n",
    "#Remove zero mass jets\n",
    "\n",
    "trainc = trainc.loc[trainc[\"m\"] > 0.08]\n",
    "testc = testc.loc[testc[\"m\"] > 0.08]\n",
    "\n",
    "#trainc = trainc.loc[trainc[\"m\"] < 15000]\n",
    "#testc = testc.loc[testc[\"m\"] < 15000]\n",
    "\n",
    "#train = train.loc[train[\"pt\"] > 10000]\n",
    "#test = test.loc[test[\"pt\"] > 10000]\n",
    "\n",
    "#train = train.loc[train[\"pt\"] < 25000]\n",
    "#test = test.loc[test[\"pt\"] < 25000]\n",
    "\n",
    "#train = train.loc[train[\"pt\"] > 25000]#.iloc(:1)\n",
    "#test = test.loc[test[\"pt\"] > 25000]\n",
    "\n",
    "#Shuffle datasets\n",
    "trainc = trainc.sample(frac=1)\n",
    "testc = testc.sample(frac=1)\n",
    "\n",
    "# Normalize\n",
    "\n",
    "# Custom normalization for AOD data\n",
    "eta_div = 5\n",
    "emfrac_div = 1.6\n",
    "negE_div = 1.6\n",
    "phi_div = 3\n",
    "m_div = 1.8\n",
    "width_div = .6\n",
    "N90_div = 20\n",
    "timing_div = 40\n",
    "hecq_div = 1\n",
    "centerlambda_div = 2\n",
    "secondlambda_div = 1\n",
    "secondR_div = .6\n",
    "larqf_div = 2.5\n",
    "pt_div = 1.2\n",
    "centroidR_div = 0.8\n",
    "area4vecm_div = 0.18\n",
    "area4vecpt_div = 0.7\n",
    "area4vec_div = 0.8\n",
    "Oot_div = 0.3\n",
    "larq_div = 0.6\n",
    "\n",
    "log_add = 100\n",
    "log_sub = 2\n",
    "\n",
    "centroidR_sub = 3\n",
    "\n",
    "area4vecm_sub = 0.15\n",
    "\n",
    "\n",
    "m_add = 1\n",
    "pt_sub = 1.3\n",
    "eta_div = 5\n",
    "phi_div = 3\n",
    "\n",
    "branches = [\"ActiveArea4vec_eta\", \"ActiveArea4vec_m\", \"ActiveArea4vec_phi\", \"ActiveArea4vec_pt\", \"Jvt\", \"JVFCorr\" ,\"JvtRpt\", \"FracSamplingMax\", \"FracSamplingMaxIndex\", \"Width\", \"EMFrac\",\"Timing\"]\n",
    "branchesreduced = [\"ActiveArea4vec_eta\", \"ActiveArea4vec_m\", \"ActiveArea4vec_phi\", \"ActiveArea4vec_pt\", \"FracSamplingMax\", \"FracSamplingMaxIndex\", \"Width\", \"EMFrac\",\"Timing\"]\n",
    "\n",
    "meansc = [trainc[k].mean() for k in branches]\n",
    "stdsc = [trainc[k].std() for k in branches]\n",
    "\n",
    "for data in [trainc,testc]:\n",
    "    #data['m'] = (np.log10(data['m'])-3.6)\n",
    "    data['m'] = np.log10(data['m'] + m_add) / m_div -1.4\n",
    "    data['eta'] = data['eta'] / eta_div\n",
    "    \n",
    "    data['phi'] = data['phi'] / phi_div\n",
    "    \n",
    "    #data['pt'] = (np.log10(data['pt']/1000-10)) -0.7\n",
    "    data['pt'] = (np.log10(data['pt']) - pt_sub) / pt_div -2.4\n",
    "    \n",
    "\n",
    "\n",
    "    data[branches[0]] = (data[branches[0]] - means[0])/stds[0]\n",
    "    data[branches[1]] = (data[branches[1]] - means[1])/stds[1]\n",
    "    data[branches[2]] = (data[branches[2]] - means[2])/stds[2]\n",
    "    data[branches[3]] = (data[branches[3]] - means[3])/stds[3]\n",
    "    data[branches[4]] = (data[branches[4]] - means[4])/stds[4]\n",
    "    data[branches[5]] = (data[branches[5]] - means[5])/stds[5]\n",
    "    data[branches[7]] = (data[branches[7]] - means[6])/stds[6]\n",
    "    data[branches[8]] = (data[branches[8]] - means[7])/stds[7]\n",
    "    data[branches[9]] = (data[branches[9]] - means[8])/stds[8]\n",
    "    data[branches[10]] = (data[branches[10]] - means[9])/stds[9]\n",
    "    data[branches[11]] = (data[branches[11]] - means[10])/stds[10]\n",
    "\n",
    "\n",
    "trainc = trainc.loc[trainc[\"m\"] > 0]\n",
    "testc = testc.loc[testc[\"m\"] > 0]\n",
    "\n",
    "trainc = trainc.drop(\"JvtRpt\",axis=1)\n",
    "testc = testc.drop(\"JvtRpt\",axis=1)\n",
    "\n",
    "\n",
    "del meansc[6:7]\n",
    "del stdsc[6:7]\n",
    "\n",
    "train_xc = trainc\n",
    "test_xc = testc\n",
    "train_yc = train_xc  # y = x since we are building and AE\n",
    "test_yc = test_xc\n",
    "\n",
    "train_dsc = TensorDataset(torch.tensor(train_xc.values), torch.tensor(train_yc.values))\n",
    "valid_dsc = TensorDataset(torch.tensor(test_xc.values), torch.tensor(test_yc.values))\n",
    "\n",
    "plt.close('all')\n",
    "unit_list = ['[GeV]', '[rad]', '[rad]', '[GeV]']\n",
    "variable_list = [r'$p_T$', r'$\\eta$', r'$\\phi$', r'$m$']\n",
    "line_style = ['--', '-']\n",
    "colors = ['red', 'c']\n",
    "markers = ['*', 's']\n",
    "\n",
    "branches=[\"pt\",\"eta\",\"phi\",\"m\",\"ActiveArea4vec_eta\", \"ActiveArea4vec_m\", \"ActiveArea4vec_phi\", \"ActiveArea4vec_pt\", \"FracSamplingMax\", \"FracSamplingMaxIndex\", \"Width\", \"EMFrac\", \"Timing\"]\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "n_bins = 200\n",
    "alph = 0.8\n",
    "for kk in np.arange(13):\n",
    "    plt.figure(kk + 13)\n",
    "    n_hist_data, bin_edges, _ = plt.hist(train[branches[kk]], color=colors[1], label='Input', alpha=1, bins=n_bins)\n",
    "    n_hist_data, bin_edges, _ = plt.hist(trainc[branches[kk]], color=colors[0], label='Input', alpha=0.6, bins=n_bins)\n",
    "    #n_hist_pred, _, _ = plt.hist(pred[:, kk], color=colors[0], label='Output', alpha=alph, bins=bin_edges)\n",
    "    plt.suptitle(branches[kk])\n",
    "    plt.xlabel(branches[kk])\n",
    "    plt.ylabel('Number of events')\n",
    "    ms.sciy()\n",
    "    # plt.yscale('log')\n",
    "    plt.legend()\n",
    "    fig_name = 'trainforever_hist_%s' % train.columns[kk]\n",
    "    #plt.savefig(curr_save_folder + fig_name)\n",
    "    plt.show()\n",
    "\"\"\"\n",
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs=2048)\n",
    "\n",
    "db = basic_data.DataBunch(train_dl, valid_dl)\n",
    "\n",
    "\n",
    "class AE_3D_200_wide(nn.Module):\n",
    "    def __init__(self, n_features=16):\n",
    "        super(AE_3D_200_wide, self).__init__()\n",
    "        self.n = 200\n",
    "        self.en1 = nn.Linear(n_features, self.n)\n",
    "        self.en2 = nn.Linear(self.n, self.n)\n",
    "        self.en3 = nn.Linear(self.n, self.n)\n",
    "        self.en4 = nn.Linear(self.n, 10)\n",
    "        self.de1 = nn.Linear(10, self.n)\n",
    "        self.de2 = nn.Linear(self.n, self.n)\n",
    "        self.de3 = nn.Linear(self.n, self.n)\n",
    "        self.de4 = nn.Linear(self.n, n_features)\n",
    "        self.tanh = nn.LeakyReLU()\n",
    "        \n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.en4(self.tanh(self.en3(self.tanh(self.en2(self.tanh(self.en1(x)))))))\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.de4(self.tanh(self.de3(self.tanh(self.de2(self.tanh(self.de1(self.tanh(x))))))))\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        return self.decode(z)\n",
    "\n",
    "    def describe(self):\n",
    "        return 'in-200-200-200-3-200-200-200-out'\n",
    "\n",
    "model = AE_3D_200_wide(15).double()\n",
    "\n",
    "#model = AE_3D_200().double()\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "\n",
    "bn_wd = False  # Don't use weight decay fpr batchnorm layers\n",
    "true_wd = True  # wd will be used for all optimizers\n",
    "wd =1e-2\n",
    "learn = basic_train.Learner(data=db, model=model, loss_func=loss_func, wd=wd, callback_fns=ActivationStats, bn_wd=bn_wd, true_wd=true_wd)\n",
    "\n",
    "train_dlc, valid_dlc = get_data(train_dsc, valid_dsc, bs=2048)\n",
    "\n",
    "dbc = basic_data.DataBunch(train_dlc, valid_dlc)\n",
    "\n",
    "modelc = AE_3D_200_wide(15).double()\n",
    "\n",
    "learnc = basic_train.Learner(data=dbc, model=modelc, loss_func=loss_func, wd=wd, callback_fns=ActivationStats, bn_wd=bn_wd, true_wd=true_wd)\n",
    "\n",
    "#model.to('cpu')\n",
    "learn.load('compressed_15_10_15')\n",
    "learnc.load('compressed_15_10_15')\n",
    "\n",
    "# Histograms\n",
    "idxs = (0, 150000)  # Choose events to compare\n",
    "data = torch.tensor(test_x[idxs[0]:idxs[1]].values)\n",
    "datac = torch.tensor(test_xc[idxs[0]:idxs[1]].values)\n",
    "\n",
    "pred = model(data).detach().numpy()\n",
    "predc = modelc(datac).detach().numpy()\n",
    "\n",
    "\n",
    "branches = [\"ActiveArea4vec_eta\", \"ActiveArea4vec_m\", \"ActiveArea4vec_phi\", \"ActiveArea4vec_pt\", \"Jvt\", \"JVFCorr\" ,\"JvtRpt\", \"FracSamplingMax\", \"FracSamplingMaxIndex\", \"Width\", \"EMFrac\",\"Timing\"]\n",
    "\n",
    "\n",
    "\n",
    "for d in [data,pred, datac, predc]:\n",
    "    #data['m'] = (np.log10(data['m'])-3.6)\n",
    "    \n",
    "    d[:, 3] = np.power(10,(d[:, 3]+1.4)*m_div)-m_add\n",
    "    \n",
    "    d[:, 1] = d[:, 1] * eta_div\n",
    "    \n",
    "    d[:, 2] = d[:, 2] *phi_div\n",
    "    \n",
    "    d[:, 0] = np.power(10,(d[:, 0]+2.4)*pt_div)+pt_sub\n",
    "\n",
    "    #d[:, 4] = (d[:, 4]*stds[0])+means[0]\n",
    "    kk = 0\n",
    "    while kk < len(means)-1:\n",
    "        d[:, 4+kk] = (d[:, 4+kk]*stds[kk])+means[kk]\n",
    "        kk+=1\n",
    "\n",
    "\n",
    "def getRatio(bin1,bin2):\n",
    "    # Sanity check\n",
    "    if len(bin1) != len(bin2):\n",
    "        print(\"Cannot make ratio!\")\n",
    "    bins = []\n",
    "    for b1,b2 in zip(bin1,bin2):\n",
    "        if b1==0 and b2==0:\n",
    "            bins.append(0.)\n",
    "        elif b2==0:\n",
    "            bins.append(None)\n",
    "        else:\n",
    "            bins.append((float(b2)-float(b1))/b1)\n",
    "    # The ratio can of course be expanded with eg. error\n",
    "    return bins    \n",
    "\n",
    "variable_list = [r'$p_T$', r'$\\eta$', r'$\\phi$', r'$m$']+branchesreduced\n",
    "\n",
    "branches = [\"m\",\"eta\",\"phi\",\"pt\"]+branchesreduced\n",
    "\n",
    "alph = 0.8\n",
    "n_bins = 50\n",
    "for kk in []:\n",
    "#for kk in range(13):\n",
    "    plt.figure(kk + 13)\n",
    "    fig, axs = plt.subplots(ncols=3,nrows=2, figsize=(10,6),gridspec_kw={'height_ratios': [3, 1]}, sharex=True, sharey='row')\n",
    "    n_hist_data, bin_edges, _ = axs[0][0].hist(data[:, kk], color='black', label='Input', alpha=1, bins=n_bins, histtype=\"step\",linewidth=2)\n",
    "    n_hist_pred, _, _ = axs[0][0].hist(pred[:, kk], color=colors[0], label='Output', alpha=1, bins=bin_edges, histtype='step',linewidth=1.5)\n",
    "    n_hist_datac, bin_edgesc, _ = axs[0][1].hist(datac[:, kk], color='green', label='Compressed Input', alpha=1, bins=bin_edges, histtype=\"step\",linewidth=2,linestyle=('dashed'))\n",
    "    n_hist_predc, _, _ = axs[0][1].hist(predc[:, kk], color='orange', label='Compressed Output', alpha=1, bins=bin_edges, histtype='step',linewidth=1,linestyle=('dashed'))\n",
    "    \n",
    "    _, _, _ = axs[0][2].hist(data[:, kk], color='black', label='Input', alpha=1, bins=bin_edges, histtype=\"step\",linewidth=2)\n",
    "    _, _, _ = axs[0][2].hist(pred[:, kk], color=colors[0], label='Output', alpha=1, bins=bin_edges, histtype='step',linewidth=1)\n",
    "    _, _, _ = axs[0][2].hist(datac[:, kk], color='green', label='Compressed Input', alpha=1, bins=bin_edges, histtype=\"step\",linewidth=2,linestyle=('dashed'))\n",
    "    _, _, _ = axs[0][2].hist(predc[:, kk], color='orange', label='Compressed Output', alpha=1, bins=bin_edges, histtype='step',linewidth=1,linestyle=('dashed'))\n",
    "    \n",
    "    #plt.suptitle(train_x.columns[kk])\n",
    "    axs[1][1].set(xlabel=variable_list[kk])\n",
    "    axs[0][0].set(ylabel='Number of events')\n",
    "    ms.sciy()\n",
    "    # plt.yscale('log')\n",
    "    #axs[0][2].legend(loc=8,framealpha=1)\n",
    "    \n",
    "    #axs[0][1].legend(loc='upper center', bbox_to_anchor=(0.5, 1.5), fancybox=True, shadow=True)\n",
    "    \n",
    "    rat = getRatio(n_hist_data,n_hist_pred)\n",
    "    ratc = getRatio(n_hist_datac,n_hist_predc)\n",
    "    axs[1][0].step(bin_edges[:-1],rat, color='black',linewidth=1)\n",
    "    axs[1][1].step(bin_edges[:-1],ratc, color='orange',linewidth=1)\n",
    "    axs[1][0].set(ylabel='Residual')\n",
    "    axs[1][0].set( ylim=(-0.5, 0.5))\n",
    "    axs[1][1].set( ylim=(-0.5, 0.5))\n",
    "    axs[1][0].ticklabel_format(style='plain')\n",
    "    axs[1][1].ticklabel_format(style='plain')\n",
    "    \n",
    "    axs[1][2].step(bin_edges[:-1],rat, color='black',linewidth=1)\n",
    "    axs[1][2].step(bin_edges[:-1],ratc, color='orange',linewidth=1)\n",
    "    axs[1][2].set( ylim=(-0.5, 0.5))\n",
    "    axs[1][2].ticklabel_format(style='plain')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig_name = 'trainforever_hist_%s' % train.columns[kk]\n",
    "    \n",
    "    plt.savefig(\"13_10_comp_histogram_\" + train_x.columns[kk])\n",
    "    plt.show()\n",
    "\n",
    "plt.close('all')\n",
    "unit_list = ['[GeV]', '[rad]', '[rad]', '[GeV]']\n",
    "variable_list = [r'$m$', r'$p_T$', r'$\\phi$', r'$\\eta$']\n",
    "line_style = ['--', '-']\n",
    "colors = ['orange', 'c']\n",
    "markers = ['*', 's']\n",
    "alph = 0.5\n",
    "n_bins = 200\n",
    "\n",
    "residual_strings = [r'$(m_{comp} - m_{raw}) / m_{raw}$',\n",
    "                        r'$(\\eta_{comp} - \\eta_{raw}) / \\eta_{raw}$',\n",
    "                        r'$(\\phi_{comp} - \\phi_{raw}) / \\phi_{raw}$',\n",
    "                        r'$(p_{T,comp} - p_{T,raw}) / p_{T,raw}$']\n",
    "\n",
    "\n",
    "residuals = (pred - data.detach().numpy())/data.detach().numpy()\n",
    "residualsc = (predc - datac.detach().numpy())/datac.detach().numpy()\n",
    "\n",
    "doubleresidual = (predc - data.detach().numpy())/data.detach().numpy()\n",
    "\n",
    "#residuals = residuals.loc[residuals[\"m\"] >= 1.]\n",
    "#residualsmc = residualsmc.loc[residualsmc[\"m\"] >= 1.]\n",
    "\n",
    "#print(residuals.head(100))\n",
    "\n",
    "#dfmc = dfmc.loc[dfmc[\"pt\"] != 0.0]\n",
    "\n",
    "#respt = (np.array(pt)-np.array(ptc))/np.array(pt)\n",
    "\n",
    "#respt = [(dfc[\"pt\"][i]-df[\"pt\"][i])/df[\"pt\"][i]]\n",
    "\n",
    "branches = [\"m\",\"eta\",\"phi\",\"pt\"]+branchesreduced\n",
    "\n",
    "#fig,axs = plt.subplots(1,13, figsize=(16,5))\n",
    "\n",
    "n_bins = 50\n",
    "axisrange=(-0.02,0.02)\n",
    "\n",
    "for kk in np.arange(15):\n",
    "    plt.figure(figsize=(5,5))\n",
    "    n_hist_pred, bin_edges, _ = plt.hist(\n",
    "        doubleresidual[:, kk], label='Residuals compressed', alpha=1, bins=n_bins, range=axisrange, density=True, color=\"black\", histtype='step', linewidth=2)\n",
    "    #n_hist_pred, bin_edges, _ = plt.hist(\n",
    "    #    residualsc[:, kk], label='Residuals', alpha=1, bins=n_bins, range=axisrange, density=True, histtype='step', color='orange', linewidth=1.5)\n",
    "    #axs[kk].xlabel(branches[kk])  # (train.columns[kk], train.columns[kk], train.columns[kk]))\n",
    "    #plt.yscale(\"log\")\n",
    "    #ms.sciy()\n",
    "    #plt.yscale('log')\n",
    "    std = np.std(residuals[:, kk])\n",
    "    std_err = utils.std_error(residuals[:, kk])\n",
    "    mean = np.nanmean(residuals[:, kk])\n",
    "    sem = stats.sem(residuals[:, kk], nan_policy='omit')\n",
    "    #stdmc = np.std(residualsc[:, kk])\n",
    "    #std_errmc = utils.std_error(residualsc[:, kk])\n",
    "    #meanmc = np.nanmean(residualsc[:, kk])\n",
    "    #semmc = stats.sem(residualsc[:, kk], nan_policy='omit')\n",
    "    axes = plt.gca()\n",
    "    #text = 'Mean = %f$\\pm$%f\\n$\\sigma$ = %f$\\pm$%f\\nComp. mean = %f$\\pm$%f\\nComp. $\\sigma$ = %f$\\pm$%f' % (mean, sem, std, std_err, meanmc, semmc, stdmc, std_errmc)\n",
    "    #axes.annotate(text, (0,0), (0, -60), xycoords='axes fraction', textcoords='offset points', va='top', fontsize='large')\n",
    "    axes.set(xlabel=branches[kk])\n",
    "    #axes.suptitle(branches[kk])\n",
    "    #axes.text(.1, .8, 'Mean = %f$\\pm$%f\\n$\\sigma$ = %f$\\pm$%f' % (mean, sem, std, std_err), bbox={'facecolor': 'white', 'alpha': 0.7, 'pad': 10},\n",
    "    #         horizontalalignment='center', verticalalignment='center', transform=axes.transAxes, fontsize=12)\n",
    "    #fig_name = 'trainforever_residual_%s' % branches[kk]\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('15_10_doubleresidual_'+branches[kk])\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
