{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "BIN = '../'\n",
    "sys.path.append(BIN)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from fastai import basic_train, basic_data\n",
    "from fastai.callbacks import ActivationStats\n",
    "import fastai\n",
    "\n",
    "from nn_utils import AE_3D_200\n",
    "\n",
    "import tracemalloc\n",
    "\n",
    "class RMSELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss,self).__init__()\n",
    "\n",
    "    def forward(self,x,y):\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = torch.sqrt(criterion(x, y))\n",
    "        return loss\n",
    "    \n",
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )\n",
    "\n",
    "model = AE_3D_200()\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "bn_wd = False  # Don't use weight decay fpr batchnorm layers\n",
    "true_wd = True  # wd will be used for all optimizers\n",
    "wd = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/skelpdar/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py:1848 Size: 31006592 bytes\n",
      "/home/skelpdar/anaconda3/lib/python3.7/site-packages/pandas/io/pickle.py:181 Size: 15505117 bytes\n",
      "/home/skelpdar/anaconda3/lib/python3.7/site-packages/IPython/core/compilerop.py:100 Size: 16411 bytes\n",
      "/home/skelpdar/anaconda3/lib/python3.7/linecache.py:137 Size: 13722 bytes\n",
      "/home/skelpdar/anaconda3/lib/python3.7/codeop.py:133 Size: 6687 bytes\n"
     ]
    }
   ],
   "source": [
    "tracemalloc.start()\n",
    "\n",
    "# Load data\n",
    "train = pd.read_pickle(BIN + 'processed_data/train.pkl')\n",
    "test = pd.read_pickle(BIN + 'processed_data/test.pkl')\n",
    "n_features = len(train.loc[0])\n",
    "\n",
    "# Normalize\n",
    "train_mean = train.mean()\n",
    "train_std = train.std()\n",
    "\n",
    "train = (train - train_mean) / train_std\n",
    "test = (test - train_mean) / train_std\n",
    "\n",
    "snapshot = tracemalloc.take_snapshot()\n",
    "top_stats = snapshot.statistics('lineno')\n",
    "tracemalloc.stop()\n",
    "# pick the biggest memory block\n",
    "stat = top_stats[0:5]\n",
    "for s in stat:\n",
    "    print(str(s.traceback) + \" Size: \"+ str(s.size) + \" bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AE_3D_200(\n",
       "  (en1): Linear(in_features=4, out_features=200, bias=True)\n",
       "  (en2): Linear(in_features=200, out_features=100, bias=True)\n",
       "  (en3): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (en4): Linear(in_features=50, out_features=3, bias=True)\n",
       "  (de1): Linear(in_features=3, out_features=50, bias=True)\n",
       "  (de2): Linear(in_features=50, out_features=100, bias=True)\n",
       "  (de3): Linear(in_features=100, out_features=200, bias=True)\n",
       "  (de4): Linear(in_features=200, out_features=4, bias=True)\n",
       "  (tanh): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = TensorDataset(torch.tensor(train_x.values), torch.tensor(train_y.values))\n",
    "valid_ds = TensorDataset(torch.tensor(test_x.values), torch.tensor(test_y.values))\n",
    "\n",
    "\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs=256)\n",
    "\n",
    "db = basic_data.DataBunch(train_dl, valid_dl)\n",
    "\n",
    "learn = basic_train.Learner(data=db, model=model, loss_func=loss_func, wd=wd, callback_fns=ActivationStats, bn_wd=bn_wd, true_wd=true_wd)\n",
    "\n",
    "#Choose the trained network here, loads from models/\n",
    "learn.load('AE_3D_200_no1cycle_trainforever')\n",
    "\n",
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of events: 387581\n",
      "4 memory blocks: 0.3 KiB\n",
      "  File \"/home/skelpdar/anaconda3/lib/python3.7/codeop.py\", line 133\n",
      "    codeob = compile(source, filename, symbol, self.flags, 1)\n"
     ]
    }
   ],
   "source": [
    "number_of_events = torch.tensor(test.values).size()[0]\n",
    "print(\"Number of events: \" + str(number_of_events))\n",
    "\n",
    "tracemalloc.start()\n",
    "\n",
    "compressed = learn.model.encode(torch.tensor(test.values)).detach().numpy()\n",
    "\n",
    "snapshot = tracemalloc.take_snapshot()\n",
    "top_stats = snapshot.statistics('traceback')\n",
    "\n",
    "# pick the biggest memory block\n",
    "stat = top_stats[0]\n",
    "print(\"%s memory blocks: %.1f KiB\" % (stat.count, stat.size / 1024))\n",
    "for line in stat.traceback.format():\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
