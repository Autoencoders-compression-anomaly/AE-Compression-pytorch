{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import click\n",
    "import pprint\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "input_folder = '/home/honey/cern/July/datasets/bsm/bsm_full/'\n",
    "# input_folder = '/home/honey/cern/July/datasets/sm/single_higgs_10fb.csv'\n",
    "\n",
    "file_list = os.listdir(input_folder)\n",
    "file_list.sort()\n",
    "# file_list = ['ww_10fb.csv']\n",
    "\n",
    "\n",
    "for file in file_list:\n",
    "    input_path = os.path.join(input_folder, file) \n",
    "\n",
    "    data = []    \n",
    "    print('Reading data at ', input_path)\n",
    "    with open(input_path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            line = line.replace(';', ',')\n",
    "            line = line.rstrip(',\\n')\n",
    "            line = line.split(',')\n",
    "            data.append(line)\n",
    "\n",
    "    #Find the longest line in the data \n",
    "    longest_line = max(data, key = len)\n",
    "    \n",
    "    #Set the maximum number of columns\n",
    "    max_col_num = len(longest_line)\n",
    "\n",
    "    #Set the columns names\n",
    "    col_names = ['event_ID', 'process_ID', 'event_weight', 'MET', 'MET_Phi']\n",
    "    meta_cols = col_names.copy()\n",
    "\n",
    "    for i in range(1, (int((max_col_num-5)/5))+1):\n",
    "        col_names.append('obj'+str(i))\n",
    "        col_names.append('E'+str(i))\n",
    "        col_names.append('pt'+str(i))\n",
    "        col_names.append('eta'+str(i))\n",
    "        col_names.append('phi'+str(i))\n",
    "\n",
    "    #Create a dataframe from the list, using the column names from before\n",
    "\n",
    "    print('Processing the data..')\n",
    "    df = pd.DataFrame(data, columns=col_names)\n",
    "    df.fillna(value=np.nan, inplace=True)\n",
    "\n",
    "    x_train_df = pd.DataFrame(df.values, columns=col_names)\n",
    "    x_train_df.fillna(value=0, inplace=True)\n",
    "\n",
    "    x_train_df = x_train_df.drop(columns=meta_cols)\n",
    "    \n",
    "    from collections import OrderedDict \n",
    "    particle_list = {}\n",
    "    for i in range(len(x_train_df)):\n",
    "        for j in x_train_df.loc[i].keys():\n",
    "            if 'obj' in j:\n",
    "                if x_train_df.loc[i][j] not in particle_list.keys():\n",
    "                    particle_list[x_train_df.loc[i][j]] = 1\n",
    "                else:\n",
    "                    particle_list[x_train_df.loc[i][j]] += 1\n",
    "    particle_list.pop(0)\n",
    "    print(particle_list)\n",
    "    \n",
    "    particle_list_sorted = OrderedDict(sorted(particle_list.items(), key = lambda x:x[1], reverse=True))\n",
    "    \n",
    "    print(dict(particle_list_sorted.items()))\n",
    "    \n",
    "    save = True\n",
    "    # plt.bar(range(len(particle_list_sorted)), list(particle_list_sorted.values()), align='center')\n",
    "    # plt.xticks(range(len(particle_list_sorted)), list(particle_list_sorted.keys()))\n",
    "\n",
    "    x = list(particle_list_sorted.keys())\n",
    "    y = list(particle_list_sorted.values())\n",
    "\n",
    "    N = len(x)\n",
    "\n",
    "    ind = np.arange(N)  # the x locations for the groups\n",
    "    width = 0.75       # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,5), dpi = 100)\n",
    "    rects1 = ax.bar(ind, y, width)\n",
    "\n",
    "    # add some text for labels, title and axes ticks\n",
    "    ax.set_ylabel('Number of particles')\n",
    "    plt.xticks(range(len(particle_list_sorted)), list(particle_list_sorted.keys()))\n",
    "    plt.title(input_path.split('/')[-1], fontsize=15)\n",
    "    \n",
    "    total = np.sum(y)\n",
    "    particle_percentage = {}\n",
    "    for key in particle_list_sorted.keys():\n",
    "        particle_percentage[key] = '%.2f'%((particle_list_sorted[key] * 100) / total)\n",
    "    \n",
    "    plt.xlabel(particle_percentage, fontsize=12)\n",
    "\n",
    "    def autolabel(rects):\n",
    "        \"\"\"\n",
    "        Attach a text label above each bar displaying its height\n",
    "        \"\"\"\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.text(rect.get_x() + rect.get_width()/2., 1*height,\n",
    "                    '%d' % int(height),\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "    autolabel(rects1)\n",
    "    if save:\n",
    "        plt.savefig(input_path.split('/')[-1][:-4] + '_particle_distribution.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all(data, logy=True):\n",
    "    for key in data.keys():\n",
    "        plt.figure()\n",
    "        plt.hist(data[key], bins=200)\n",
    "        plt.xlabel(str(key))\n",
    "        plt.ylabel('Number of particles')\n",
    "        plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import click\n",
    "import pprint\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "input_folder = '/home/honey/cern/July/datasets/sm/'\n",
    "# input_folder = '/home/honey/cern/July/datasets/sm/single_higgs_10fb.csv'\n",
    "\n",
    "# file_list = os.listdir(input_folder)\n",
    "# file_list.sort()\n",
    "file_list = ['zw_10fb.csv']\n",
    "save = True\n",
    "\n",
    "for file in file_list:\n",
    "    input_path = os.path.join(input_folder, file) \n",
    "\n",
    "    data = []    \n",
    "    print('Reading data at ', input_path)\n",
    "    with open(input_path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            line = line.replace(';', ',')\n",
    "            line = line.rstrip(',\\n')\n",
    "            line = line.split(',')\n",
    "            data.append(line)\n",
    "\n",
    "    #Find the longest line in the data \n",
    "    longest_line = max(data, key = len)\n",
    "    \n",
    "    #Set the maximum number of columns\n",
    "    max_col_num = len(longest_line)\n",
    "\n",
    "    #Set the columns names\n",
    "    col_names = ['event_ID', 'process_ID', 'event_weight', 'MET', 'MET_Phi']\n",
    "    meta_cols = col_names.copy()\n",
    "\n",
    "    for i in range(1, (int((max_col_num-5)/5))+1):\n",
    "        col_names.append('obj'+str(i))\n",
    "        col_names.append('E'+str(i))\n",
    "        col_names.append('pt'+str(i))\n",
    "        col_names.append('eta'+str(i))\n",
    "        col_names.append('phi'+str(i))\n",
    "\n",
    "    #Create a dataframe from the list, using the column names from before\n",
    "\n",
    "    print('Processing the data..')\n",
    "    df = pd.DataFrame(data, columns=col_names)\n",
    "    df.fillna(value=np.nan, inplace=True)\n",
    "\n",
    "    x_train_df = pd.DataFrame(df.values, columns=col_names)\n",
    "    x_train_df.fillna(value=0, inplace=True)\n",
    "\n",
    "    x_train_df = x_train_df.drop(columns=meta_cols)\n",
    "    x = x_train_df.values.reshape([x_train_df.shape[0]*x_train_df.shape[1]//5,5])\n",
    "    del x_train_df\n",
    "    lst = []\n",
    "    for i in range(x.shape[0]):\n",
    "        if (x[i] == 0).all():\n",
    "            lst.append(i)\n",
    "    data = np.delete(x, lst, 0)\n",
    "    col_names = ['obj', 'E', 'pt', 'eta', 'phi']\n",
    "    data = pd.DataFrame(data, columns=col_names)\n",
    "    data_4d_df = data.drop(columns='obj')\n",
    "    data_4d_df = data_4d_df.fillna(0)\n",
    "    data_4d_df = data_4d_df.astype('float32')\n",
    "    data_4d_df['E'] = data_4d_df['E'] / 1000.0\n",
    "    data_4d_df['pt'] = data_4d_df['pt'] / 1000.0\n",
    "    \n",
    "#     from collections import OrderedDict \n",
    "#     particle_list = {}\n",
    "#     for i in range(len(x_train_df)):\n",
    "#         for j in x_train_df.loc[i].keys():\n",
    "#             if 'obj' in j:\n",
    "#                 if x_train_df.loc[i][j] not in particle_list.keys():\n",
    "#                     particle_list[x_train_df.loc[i][j]] = 1\n",
    "#                 else:\n",
    "#                     particle_list[x_train_df.loc[i][j]] += 1\n",
    "    \n",
    "#     plot_all(data_4d_df)\n",
    "    for key in data_4d_df.keys():\n",
    "        plt.figure()\n",
    "        plt.hist(data_4d_df[key], bins=200)\n",
    "        plt.xlabel(str(key))\n",
    "        plt.ylabel('Number of particles')\n",
    "        plt.yscale('log')\n",
    "            \n",
    "            \n",
    "        if save:\n",
    "            plt.savefig(input_path.split('/')[-1][:-4] + '_' + str(key) + '_data_distribution.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
